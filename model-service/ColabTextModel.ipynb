{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-00-title",
   "metadata": {},
   "source": [
    "# AI Text Detector — Upgraded Training (RoBERTa + Multi-Dataset)\n",
    "\n",
    "**Before running:** `Runtime → Change runtime type → GPU → A100 → Save`\n",
    "\n",
    "### What's improved over v1\n",
    "| | v1 | v2 |\n",
    "|---|---|---|\n",
    "| Base model | DistilBERT (67M) | RoBERTa-base (125M) |\n",
    "| Max tokens | 256 | 512 |\n",
    "| Training data | ~85K HC3 + 60K RAID | ~85K HC3 + 300K RAID (stratified across ALL generators) |\n",
    "| AI generators covered | ChatGPT only | GPT-4, GPT-3.5, GPT-2, Llama-2 (7/13/70B), Mistral, Cohere, BLOOM, + adversarial attacks |\n",
    "| Epochs | 3 | 4 with early stopping |\n",
    "| Precision | fp16 | bf16 (more stable on A100) |\n",
    "| LR schedule | linear | cosine with warmup |\n",
    "| Label smoothing | no | 0.05 (reduces overconfidence) |\n",
    "| Temperature | hardcoded 1.8 | auto-calibrated + saved to JSON |\n",
    "\n",
    "**Expected training time on A100:** ~2–3 hours for 4 epochs over ~385K examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-01-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "SAVE_DIR = '/content/drive/MyDrive/ai-detector-v2'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f'Drive mounted. Model will save to: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate evaluate scikit-learn torch seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ─────────────────────────────────────────────────────────────\n",
    "# Change BASE_MODEL here to try different architectures:\n",
    "#   'roberta-base'                 — best accuracy / speed tradeoff (recommended)\n",
    "#   'microsoft/deberta-v3-base'    — highest accuracy, ~2x slower\n",
    "#   'distilbert-base-uncased'      — fastest, lowest accuracy\n",
    "\n",
    "BASE_MODEL   = 'roberta-base'\n",
    "MAX_LEN      = 512\n",
    "EPOCHS       = 4\n",
    "BATCH_SIZE   = 32   # per device; effective batch = 32 * grad_accum_steps\n",
    "GRAD_ACCUM   = 2    # effective batch = 64\n",
    "LR           = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.06\n",
    "LABEL_SMOOTH = 0.05\n",
    "SEED         = 42\n",
    "\n",
    "# How many rows to sample from RAID (5.6M available — more = better, but slower)\n",
    "RAID_AI_SAMPLE    = 250_000   # AI-generated rows (stratified across all generators)\n",
    "RAID_HUMAN_SAMPLE = 100_000   # Human rows from RAID\n",
    "RAID_ADV_SAMPLE   =  50_000   # Adversarially-attacked AI rows (robustness)\n",
    "\n",
    "print('Config loaded.')\n",
    "print(f'  Base model : {BASE_MODEL}')\n",
    "print(f'  Max tokens : {MAX_LEN}')\n",
    "print(f'  Epochs     : {EPOCHS}')\n",
    "print(f'  Save dir   : {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04-hc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ── HC3 — ChatGPT vs human answers (Reddit, medicine, finance, etc.) ──────────\n",
    "hc3_raw = pd.read_parquet(\n",
    "    'hf://datasets/Hello-SimpleAI/HC3@refs/convert/parquet/all/train/0000.parquet'\n",
    ")\n",
    "\n",
    "human_hc3 = (\n",
    "    hc3_raw[['human_answers']]\n",
    "    .explode('human_answers')\n",
    "    .rename(columns={'human_answers': 'text'})\n",
    "    .assign(label=0, source='hc3')\n",
    ")\n",
    "ai_hc3 = (\n",
    "    hc3_raw[['chatgpt_answers']]\n",
    "    .explode('chatgpt_answers')\n",
    "    .rename(columns={'chatgpt_answers': 'text'})\n",
    "    .assign(label=1, source='hc3')\n",
    ")\n",
    "\n",
    "hc3 = pd.concat([human_hc3, ai_hc3], ignore_index=True)\n",
    "hc3['text'] = hc3['text'].astype(str).str.strip()\n",
    "hc3 = hc3[hc3['text'].str.len() > 50].reset_index(drop=True)\n",
    "\n",
    "print('HC3:', hc3['label'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-05-raid",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ── RAID — multi-generator, multi-domain ──────────────────────────────────────\n",
    "# Generators: gpt-4, gpt-3.5-turbo, gpt-2, llama-2 (7/13/70b), mistral-7b,\n",
    "#             cohere, bloom-7b1, davinci-003, and more\n",
    "# Domains: news, reddit, recipes, abstracts, reviews, wiki, poetry, etc.\n",
    "# Attacks: none, homoglyph, paraphrase, whitespace, etc.\n",
    "\n",
    "print('Loading RAID dataset (this may take a few minutes)...')\n",
    "raid_raw = load_dataset('liamdugan/raid', split='train')\n",
    "\n",
    "# Convert to pandas in chunks to avoid OOM\n",
    "raid_df = raid_raw.to_pandas()\n",
    "print(f'RAID loaded: {len(raid_df):,} rows')\n",
    "print('Generators:', raid_df['model'].value_counts().head(15).to_dict())\n",
    "print('Domains:', raid_df['domain'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06-raid-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sample RAID strategically ─────────────────────────────────────────────────\n",
    "\n",
    "# 1. Clean AI rows (no adversarial attack) — stratified across all generators\n",
    "clean_ai = raid_df[(raid_df['model'] != 'human') & (raid_df['attack'] == 'none')]\n",
    "generators = clean_ai['model'].unique()\n",
    "per_gen = RAID_AI_SAMPLE // len(generators)\n",
    "\n",
    "clean_ai_sample = (\n",
    "    clean_ai\n",
    "    .groupby('model', group_keys=False)\n",
    "    .apply(lambda g: g.sample(min(len(g), per_gen), random_state=SEED))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(f'Clean AI samples: {len(clean_ai_sample):,}')\n",
    "print('Per generator:', clean_ai_sample['model'].value_counts().to_dict())\n",
    "\n",
    "# 2. Adversarially-attacked AI rows — teaches model to resist paraphrasing tricks\n",
    "adv_ai = raid_df[(raid_df['model'] != 'human') & (raid_df['attack'] != 'none')]\n",
    "adv_ai_sample = adv_ai.sample(min(len(adv_ai), RAID_ADV_SAMPLE), random_state=SEED)\n",
    "print(f'Adversarial AI samples: {len(adv_ai_sample):,}')\n",
    "print('Attack types:', adv_ai_sample['attack'].value_counts().to_dict())\n",
    "\n",
    "# 3. Human rows from RAID\n",
    "human_raid = raid_df[raid_df['model'] == 'human']\n",
    "human_raid_sample = human_raid.sample(min(len(human_raid), RAID_HUMAN_SAMPLE), random_state=SEED)\n",
    "print(f'Human RAID samples: {len(human_raid_sample):,}')\n",
    "\n",
    "# Build RAID dataframe\n",
    "raid_ai = pd.concat([clean_ai_sample, adv_ai_sample], ignore_index=True)\n",
    "raid_ai_df = raid_ai[['generation']].rename(columns={'generation': 'text'}).assign(label=1, source='raid')\n",
    "raid_human_df = human_raid_sample[['generation']].rename(columns={'generation': 'text'}).assign(label=0, source='raid')\n",
    "\n",
    "raid = pd.concat([raid_ai_df, raid_human_df], ignore_index=True)\n",
    "raid['text'] = raid['text'].astype(str).str.strip()\n",
    "raid = raid[raid['text'].str.len() > 50].reset_index(drop=True)\n",
    "\n",
    "print(f'\\nRAID final: {raid[\"label\"].value_counts().to_dict()}')\n",
    "\n",
    "# Free the huge raw dataframe\n",
    "del raid_df, raid_raw, clean_ai, adv_ai, clean_ai_sample, adv_ai_sample, human_raid, human_raid_sample\n",
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-07-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Combine all datasets ──────────────────────────────────────────────────────\n",
    "combined = pd.concat([hc3[['text','label','source']], raid], ignore_index=True)\n",
    "\n",
    "# Drop empty/whitespace rows\n",
    "combined = combined[combined['text'].str.strip().str.len() > 50].reset_index(drop=True)\n",
    "\n",
    "# Shuffle\n",
    "combined = combined.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print('=== Combined dataset ===')\n",
    "print(f'Total rows  : {len(combined):,}')\n",
    "print(f'Human (0)   : {(combined[\"label\"]==0).sum():,}')\n",
    "print(f'AI (1)      : {(combined[\"label\"]==1).sum():,}')\n",
    "print(f'Sources     : {combined[\"source\"].value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-08-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    combined, test_size=0.1, random_state=SEED, stratify=combined['label']\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.1, random_state=SEED, stratify=train_df['label']\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df   = val_df.reset_index(drop=True)\n",
    "test_df  = test_df.reset_index(drop=True)\n",
    "\n",
    "print(f'Train : {len(train_df):,}  |  Val : {len(val_df):,}  |  Test : {len(test_df):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-09-tokenize",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[['text','label']])\n",
    "val_ds   = Dataset.from_pandas(val_df[['text','label']])\n",
    "test_ds  = Dataset.from_pandas(test_df[['text','label']])\n",
    "\n",
    "train_tok = train_ds.map(tokenize, batched=True, batch_size=1000, num_proc=2)\n",
    "val_tok   = val_ds.map(tokenize,   batched=True, batch_size=1000, num_proc=2)\n",
    "test_tok  = test_ds.map(tokenize,  batched=True, batch_size=1000, num_proc=2)\n",
    "\n",
    "cols = ['input_ids', 'attention_mask', 'label']\n",
    "train_tok.set_format('torch', columns=cols)\n",
    "val_tok.set_format('torch',   columns=cols)\n",
    "test_tok.set_format('torch',  columns=cols)\n",
    "\n",
    "print('Tokenization done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    num_labels=2,\n",
    "    id2label={0: 'human', 1: 'ai'},\n",
    "    label2id={'human': 0, 'ai': 1},\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing — halves VRAM at the cost of ~20% speed\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model: {BASE_MODEL}  |  Params: {total_params/1e6:.1f}M')\n",
    "\n",
    "# ── Metrics ──\n",
    "acc_metric = evaluate.load('accuracy')\n",
    "f1_metric  = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = acc_metric.compute(predictions=preds, references=labels)['accuracy']\n",
    "    f1  = f1_metric.compute(predictions=preds, references=labels, average='macro')['f1']\n",
    "    f1_ai    = f1_metric.compute(predictions=preds, references=labels, average='binary', pos_label=1)['f1']\n",
    "    f1_human = f1_metric.compute(predictions=preds, references=labels, average='binary', pos_label=0)['f1']\n",
    "    return {\n",
    "        'accuracy'  : acc,\n",
    "        'f1_macro'  : f1,\n",
    "        'f1_ai'     : f1_ai,\n",
    "        'f1_human'  : f1_human,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11-train-args",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "\n",
    "    # Batch & gradient\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    # Regularization\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    label_smoothing_factor=LABEL_SMOOTH,\n",
    "\n",
    "    # Epochs & evaluation\n",
    "    num_train_epochs=EPOCHS,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro',\n",
    "\n",
    "    # Performance\n",
    "    bf16=True,              # bf16 is more stable than fp16 on A100\n",
    "    dataloader_num_workers=4,\n",
    "    group_by_length=True,   # speeds up training by batching similar-length sequences\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=200,\n",
    "    report_to='none',       # disable wandb/mlflow\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "print('Trainer ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── TRAIN ─────────────────────────────────────────────────────────────────────\n",
    "# On A100 with ~385K examples, expect ~2-3 hours for 4 epochs.\n",
    "# Early stopping will halt if val f1_macro stops improving.\n",
    "\n",
    "trainer.train()\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13-evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ── Full evaluation on held-out test set ─────────────────────────────────────\n",
    "test_pred = trainer.predict(test_tok)\n",
    "preds     = np.argmax(test_pred.predictions, axis=-1)\n",
    "labels    = test_pred.label_ids\n",
    "\n",
    "print('=== Test Set Results ===')\n",
    "print(classification_report(labels, preds, target_names=['Human', 'AI'], digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Human','AI'], yticklabels=['Human','AI'])\n",
    "plt.title('Confusion Matrix — Test Set')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "# ── Temperature scaling calibration on validation set ─────────────────────────\n",
    "# This makes the model's confidence scores better calibrated (less overconfident).\n",
    "\n",
    "val_pred   = trainer.predict(val_tok)\n",
    "val_logits = torch.tensor(val_pred.predictions, dtype=torch.float32)\n",
    "val_labels = torch.tensor(val_pred.label_ids,   dtype=torch.long)\n",
    "\n",
    "class TemperatureScaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_temp = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, logits):\n",
    "        return logits / torch.exp(self.log_temp)\n",
    "\n",
    "scaler = TemperatureScaler()\n",
    "opt    = torch.optim.LBFGS([scaler.log_temp], lr=0.1, max_iter=100)\n",
    "\n",
    "def calib_loss():\n",
    "    opt.zero_grad()\n",
    "    loss = F.cross_entropy(scaler(val_logits), val_labels)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "opt.step(calib_loss)\n",
    "\n",
    "T = float(torch.exp(scaler.log_temp).detach().numpy()[0])\n",
    "print(f'Calibrated temperature: {T:.4f}')\n",
    "\n",
    "# ── Save temperature alongside model so model_loader.py picks it up automatically\n",
    "config_path = f'{SAVE_DIR}/best-model/training_config.json'\n",
    "training_config = {\n",
    "    'temperature': T,\n",
    "    'base_model': BASE_MODEL,\n",
    "    'max_length': MAX_LEN,\n",
    "}\n",
    "os.makedirs(f'{SAVE_DIR}/best-model', exist_ok=True)\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "print(f'Saved training_config.json: {training_config}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save best model + tokenizer ───────────────────────────────────────────────\n",
    "BEST_MODEL_DIR = f'{SAVE_DIR}/best-model'\n",
    "\n",
    "trainer.model.save_pretrained(BEST_MODEL_DIR)\n",
    "tokenizer.save_pretrained(BEST_MODEL_DIR)\n",
    "\n",
    "print(f'Model saved to: {BEST_MODEL_DIR}')\n",
    "\n",
    "# List saved files\n",
    "for f in os.listdir(BEST_MODEL_DIR):\n",
    "    size = os.path.getsize(f'{BEST_MODEL_DIR}/{f}') / 1e6\n",
    "    print(f'  {f}  ({size:.1f} MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16-test-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sanity-check inference ────────────────────────────────────────────────────\n",
    "\n",
    "def predict(text: str, threshold: float = 0.85):\n",
    "    m = trainer.model\n",
    "    m.eval()\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=MAX_LEN)\n",
    "    inputs = {k: v.to(m.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = m(**inputs).logits[0].detach().cpu()\n",
    "    probs    = torch.softmax(logits / T, dim=-1).numpy()\n",
    "    ai_p     = float(probs[1])\n",
    "    human_p  = float(probs[0])\n",
    "    pred     = ('uncertain' if max(ai_p, human_p) < threshold\n",
    "                else ('ai' if ai_p >= human_p else 'human'))\n",
    "    print(f'AI:    {ai_p:.4f}')\n",
    "    print(f'Human: {human_p:.4f}')\n",
    "    print(f'Pred:  {pred}')\n",
    "    return {'ai_prob': ai_p, 'human_prob': human_p, 'pred': pred}\n",
    "\n",
    "print('--- Should be HUMAN ---')\n",
    "predict(\"\"\"The quick brown fox jumps over the lazy dog.\n",
    "i was just kinda sitting there and then boom it happened lol idk man it was weird.\n",
    "we went to the store and got some stuff, nothing special really.\"\"\")\n",
    "\n",
    "print('\\n--- Should be AI ---')\n",
    "predict(\"\"\"In conclusion, the integration of artificial intelligence into modern healthcare\n",
    "systems presents both significant opportunities and considerable challenges. By leveraging\n",
    "machine learning algorithms, medical professionals can enhance diagnostic accuracy,\n",
    "streamline patient care workflows, and ultimately improve patient outcomes.\n",
    "However, it is essential to address ethical considerations, data privacy concerns,\n",
    "and the need for robust validation frameworks to ensure the responsible deployment\n",
    "of these transformative technologies.\"\"\")\n",
    "\n",
    "print('\\n--- News article (should be HUMAN, not AI) ---')\n",
    "predict(\"\"\"The Federal Reserve raised interest rates by a quarter point Wednesday,\n",
    "its 10th increase since March 2022, as officials signaled they may be nearing\n",
    "the end of their aggressive campaign to bring inflation back under control.\n",
    "The decision was unanimous. The benchmark federal funds rate now sits in a\n",
    "target range of 5% to 5.25%, the highest level in 16 years.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17-download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Download the trained model as a zip ───────────────────────────────────────\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "zip_path = '/content/ai-detector-v2-export'\n",
    "shutil.make_archive(zip_path, 'zip', BEST_MODEL_DIR)\n",
    "\n",
    "zip_size = os.path.getsize(zip_path + '.zip') / 1e6\n",
    "print(f'Zip size: {zip_size:.0f} MB')\n",
    "print('Downloading...')\n",
    "files.download(zip_path + '.zip')"
   ]
  }
 ]
}
