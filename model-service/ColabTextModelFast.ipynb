{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-00-title",
   "metadata": {},
   "source": "# AI Text Detector — Fast Training (DistilBERT, ~30-45 min)\n\n**Before running:** `Runtime → Change runtime type → GPU → T4 (or A100) → Save`\n\n## Why the v2 notebook (RoBERTa) likely failed\n\n| Problem | Detail |\n|---|---|\n| Training timeout | ~385K examples × 4 epochs with RoBERTa + 512 tokens ≈ 3–6 h. Colab disconnects end training mid-epoch, leaving a partially-fitted or uncalibrated model. |\n| Class imbalance | 300K AI samples vs 150K human (≈ 2:1). Without class-weight compensation the model learns to always predict AI and gets ~70% accuracy by never saying 'human'. |\n| Slow RAID load | `to_pandas()` on 5.6 M rows uses 8–10 GB RAM and 15–20 min just loading data. |\n| MAX_LEN=512 | Self-attention is O(n²). 512 tokens is 4× slower per batch than 256. |\n\n## What this notebook does differently\n\n| | v2 (broken) | v3 (this) |\n|---|---|---|\n| Base model | RoBERTa-base (125 M) | DistilBERT-base (67 M) — 2× faster |\n| Max tokens | 512 | 256 — 4× faster attention |\n| Dataset | ~385K (unbalanced) | ~48K HC3, perfectly balanced |\n| RAID loading | full 5.6 M rows → pandas | skipped (not needed for baseline) |\n| Training time | 3–6 h | **~30–45 min on T4, ~20 min on A100** |\n| Class balance | 70 % AI / 30 % human | 50 / 50 enforced |\n| Precision | bf16 only (A100) | fp16 (works on T4 AND A100) |\n\n**Output**: a `best-model/` directory compatible with the existing `model_loader.py`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-01-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "SAVE_DIR = '/content/drive/MyDrive/ai-detector-fast'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f'Drive mounted. Model will save to: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02-install",
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q transformers datasets accelerate evaluate scikit-learn seaborn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Configuration ──────────────────────────────────────────────────────────────\n",
    "BASE_MODEL   = 'distilbert-base-uncased'  # 67M params — ~2x faster than roberta-base\n",
    "MAX_LEN      = 256    # 4x faster attention than 512; most text fits in 256 tokens\n",
    "EPOCHS       = 3\n",
    "BATCH_SIZE   = 32\n",
    "LR           = 3e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.06\n",
    "SEED         = 42\n",
    "\n",
    "# Cap per class — HC3 has ~24K AI and ~37K human rows after exploding.\n",
    "# We cap both at PER_CLASS to keep training perfectly balanced.\n",
    "PER_CLASS = 24_000\n",
    "\n",
    "print('Config loaded.')\n",
    "print(f'  Base model : {BASE_MODEL}')\n",
    "print(f'  Max tokens : {MAX_LEN}')\n",
    "print(f'  Epochs     : {EPOCHS}')\n",
    "print(f'  Save dir   : {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04-hc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ── Load HC3 (ChatGPT vs human answers from Reddit/medicine/finance/etc.) ──────\n",
    "# Loads from HuggingFace as a parquet — much faster than RAID\n",
    "print('Loading HC3...')\n",
    "hc3_raw = pd.read_parquet(\n",
    "    'hf://datasets/Hello-SimpleAI/HC3@refs/convert/parquet/all/train/0000.parquet'\n",
    ")\n",
    "print(f'HC3 raw rows: {len(hc3_raw):,}')\n",
    "\n",
    "# Explode the list columns so each answer is its own row\n",
    "human_rows = (\n",
    "    hc3_raw[['human_answers']]\n",
    "    .explode('human_answers')\n",
    "    .rename(columns={'human_answers': 'text'})\n",
    "    .assign(label=0)\n",
    ")\n",
    "ai_rows = (\n",
    "    hc3_raw[['chatgpt_answers']]\n",
    "    .explode('chatgpt_answers')\n",
    "    .rename(columns={'chatgpt_answers': 'text'})\n",
    "    .assign(label=1)\n",
    ")\n",
    "\n",
    "# Clean: strip whitespace, drop nulls, drop very short responses (< 15 words)\n",
    "for df in [human_rows, ai_rows]:\n",
    "    df['text'] = df['text'].astype(str).str.strip()\n",
    "\n",
    "human_rows = human_rows[\n",
    "    human_rows['text'].str.split().str.len() >= 15\n",
    "].dropna(subset=['text']).reset_index(drop=True)\n",
    "\n",
    "ai_rows = ai_rows[\n",
    "    ai_rows['text'].str.split().str.len() >= 15\n",
    "].dropna(subset=['text']).reset_index(drop=True)\n",
    "\n",
    "print(f'After filtering — Human: {len(human_rows):,}  AI: {len(ai_rows):,}')\n",
    "\n",
    "# ── Balance: undersample majority class to PER_CLASS ──────────────────────────\n",
    "n = min(len(human_rows), len(ai_rows), PER_CLASS)\n",
    "human_bal = human_rows.sample(n, random_state=SEED).reset_index(drop=True)\n",
    "ai_bal    = ai_rows.sample(n, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "combined = pd.concat([human_bal, ai_bal], ignore_index=True)\n",
    "combined = combined.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f'\\n=== Dataset ===')\n",
    "print(f'Total rows : {len(combined):,}')\n",
    "print(f'Human (0)  : {(combined[\"label\"]==0).sum():,}')\n",
    "print(f'AI    (1)  : {(combined[\"label\"]==1).sum():,}')\n",
    "print('Balance is exactly 50/50 ✓')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-05-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80% train / 10% val / 10% test\n",
    "train_df, test_df = train_test_split(\n",
    "    combined, test_size=0.10, random_state=SEED, stratify=combined['label']\n",
    ")\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.1111, random_state=SEED, stratify=train_df['label']\n",
    ")  # 0.1111 * 0.90 ≈ 0.10 of total\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df   = val_df.reset_index(drop=True)\n",
    "test_df  = test_df.reset_index(drop=True)\n",
    "\n",
    "print(f'Train : {len(train_df):,}  |  Val : {len(val_df):,}  |  Test : {len(test_df):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06-tokenize",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(f'Loading tokenizer: {BASE_MODEL}')\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "val_ds   = Dataset.from_pandas(val_df[['text', 'label']])\n",
    "test_ds  = Dataset.from_pandas(test_df[['text', 'label']])\n",
    "\n",
    "# num_proc=2 speeds up tokenization in Colab\n",
    "train_tok = train_ds.map(tokenize, batched=True, batch_size=1000, num_proc=2)\n",
    "val_tok   = val_ds.map(tokenize,   batched=True, batch_size=1000, num_proc=2)\n",
    "test_tok  = test_ds.map(tokenize,  batched=True, batch_size=1000, num_proc=2)\n",
    "\n",
    "cols = ['input_ids', 'attention_mask', 'label']\n",
    "train_tok.set_format('torch', columns=cols)\n",
    "val_tok.set_format('torch',   columns=cols)\n",
    "test_tok.set_format('torch',  columns=cols)\n",
    "\n",
    "print('Tokenization done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-07-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    num_labels=2,\n",
    "    id2label={0: 'human', 1: 'ai'},\n",
    "    label2id={'human': 0, 'ai': 1},\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model: {BASE_MODEL}  |  Params: {total_params/1e6:.1f}M')\n",
    "\n",
    "# ── Metrics ──\n",
    "acc_metric = evaluate.load('accuracy')\n",
    "f1_metric  = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc      = acc_metric.compute(predictions=preds, references=labels)['accuracy']\n",
    "    f1_mac   = f1_metric.compute(predictions=preds, references=labels, average='macro')['f1']\n",
    "    f1_ai    = f1_metric.compute(predictions=preds, references=labels, average='binary', pos_label=1)['f1']\n",
    "    f1_human = f1_metric.compute(predictions=preds, references=labels, average='binary', pos_label=0)['f1']\n",
    "    return {\n",
    "        'accuracy' : acc,\n",
    "        'f1_macro' : f1_mac,\n",
    "        'f1_ai'    : f1_ai,\n",
    "        'f1_human' : f1_human,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-08-trainer-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "\n",
    "    # Batch\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=64,\n",
    "\n",
    "    # Regularization\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "\n",
    "    # Epochs\n",
    "    num_train_epochs=EPOCHS,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro',\n",
    "\n",
    "    # Performance — fp16 works on T4 AND A100 (unlike bf16 which needs A100)\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    group_by_length=True,   # batches similar-length seqs together → faster\n",
    "\n",
    "    # Logging\n",
    "    logging_steps=50,\n",
    "    report_to='none',\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "# ── Time estimate ──\n",
    "steps_per_epoch = len(train_tok) // BATCH_SIZE\n",
    "total_steps     = steps_per_epoch * EPOCHS\n",
    "print(f'Train examples : {len(train_tok):,}')\n",
    "print(f'Steps / epoch  : {steps_per_epoch:,}')\n",
    "print(f'Total steps    : {total_steps:,}')\n",
    "print(f'Est. time (T4) : ~{total_steps * 0.22 / 60:.0f} min')\n",
    "print(f'Est. time (A100): ~{total_steps * 0.10 / 60:.0f} min')\n",
    "print('\\nTrainer ready — run next cell to start training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-09-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── TRAIN ─────────────────────────────────────────────────────────────────────\n",
    "# Expect ~30-45 min on T4, ~15-20 min on A100.\n",
    "# EarlyStoppingCallback will stop if val f1_macro stops improving.\n",
    "\n",
    "trainer.train()\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10-evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ── Full evaluation on held-out test set ──────────────────────────────────────\n",
    "test_pred = trainer.predict(test_tok)\n",
    "preds     = np.argmax(test_pred.predictions, axis=-1)\n",
    "labels    = test_pred.label_ids\n",
    "\n",
    "print('=== Test Set Results ===')\n",
    "print(classification_report(labels, preds, target_names=['Human', 'AI'], digits=4))\n",
    "\n",
    "# Sanity check: if precision/recall is 0 for either class, the model is broken\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "p, r, f, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "if p < 0.55 or r < 0.55:\n",
    "    print('\\n⚠️  WARNING: low precision or recall — model may be predicting only one class.')\n",
    "    print('   Check the confusion matrix. If one row is all zeros, retrain with a lower LR.')\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Human', 'AI'], yticklabels=['Human', 'AI'])\n",
    "plt.title('Confusion Matrix — Test Set')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{SAVE_DIR}/confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "# ── Temperature scaling calibration ───────────────────────────────────────────\n",
    "# Makes the model's confidence scores better calibrated (less overconfident).\n",
    "# Calibrated temperature is saved into training_config.json so model_loader.py\n",
    "# picks it up automatically — no hardcoding needed.\n",
    "\n",
    "val_pred   = trainer.predict(val_tok)\n",
    "val_logits = torch.tensor(val_pred.predictions, dtype=torch.float32)\n",
    "val_labels = torch.tensor(val_pred.label_ids,   dtype=torch.long)\n",
    "\n",
    "class TemperatureScaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_temp = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, logits):\n",
    "        return logits / torch.exp(self.log_temp)\n",
    "\n",
    "scaler = TemperatureScaler()\n",
    "opt    = torch.optim.LBFGS([scaler.log_temp], lr=0.1, max_iter=100)\n",
    "\n",
    "def calib_loss():\n",
    "    opt.zero_grad()\n",
    "    loss = F.cross_entropy(scaler(val_logits), val_labels)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "opt.step(calib_loss)\n",
    "\n",
    "T = float(torch.exp(scaler.log_temp).detach().numpy()[0])\n",
    "print(f'Calibrated temperature: {T:.4f}')\n",
    "print('(Values > 1 reduce overconfidence; < 1 increase it)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save best model + tokenizer + training config ─────────────────────────────\n",
    "BEST_MODEL_DIR = f'{SAVE_DIR}/best-model'\n",
    "os.makedirs(BEST_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Save model weights and tokenizer\n",
    "trainer.model.save_pretrained(BEST_MODEL_DIR)\n",
    "tokenizer.save_pretrained(BEST_MODEL_DIR)\n",
    "\n",
    "# Save temperature + metadata so model_loader.py picks them up\n",
    "training_config = {\n",
    "    'temperature' : T,\n",
    "    'base_model'  : BASE_MODEL,\n",
    "    'max_length'  : MAX_LEN,\n",
    "}\n",
    "config_path = f'{BEST_MODEL_DIR}/training_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(f'Model saved to: {BEST_MODEL_DIR}')\n",
    "print(f'training_config.json: {training_config}')\n",
    "print()\n",
    "for fname in sorted(os.listdir(BEST_MODEL_DIR)):\n",
    "    size = os.path.getsize(f'{BEST_MODEL_DIR}/{fname}') / 1e6\n",
    "    print(f'  {fname}  ({size:.1f} MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Sanity-check inference ────────────────────────────────────────────────────\n",
    "\n",
    "def predict_text(text: str, temp: float = T, threshold: float = 0.85):\n",
    "    m = trainer.model\n",
    "    m.eval()\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors='pt', truncation=True, max_length=MAX_LEN\n",
    "    )\n",
    "    inputs = {k: v.to(m.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = m(**inputs).logits[0].detach().cpu()\n",
    "    probs   = torch.softmax(logits / temp, dim=-1).numpy()\n",
    "    ai_p    = float(probs[1])\n",
    "    human_p = float(probs[0])\n",
    "    pred    = (\n",
    "        'uncertain' if max(ai_p, human_p) < threshold\n",
    "        else ('ai' if ai_p >= human_p else 'human')\n",
    "    )\n",
    "    print(f'  AI: {ai_p:.4f}  Human: {human_p:.4f}  → {pred}')\n",
    "    return {'ai_prob': ai_p, 'human_prob': human_p, 'pred': pred}\n",
    "\n",
    "\n",
    "print('--- Should be HUMAN (casual) ---')\n",
    "predict_text(\n",
    "    \"i was just kinda sitting there and then boom it happened lol idk man it was weird \"\n",
    "    \"tbh i dont even know what to say about it, really strange experience overall\"\n",
    ")\n",
    "\n",
    "print('\\n--- Should be HUMAN (news) ---')\n",
    "predict_text(\n",
    "    \"The Federal Reserve raised interest rates by a quarter point Wednesday, its 10th increase \"\n",
    "    \"since March 2022, as officials signaled they may be nearing the end of their aggressive \"\n",
    "    \"campaign to bring inflation back under control. The decision was unanimous.\"\n",
    ")\n",
    "\n",
    "print('\\n--- Should be AI (formal essay conclusion) ---')\n",
    "predict_text(\n",
    "    \"In conclusion, the integration of artificial intelligence into modern healthcare systems \"\n",
    "    \"presents both significant opportunities and considerable challenges. By leveraging machine \"\n",
    "    \"learning algorithms, medical professionals can enhance diagnostic accuracy, streamline \"\n",
    "    \"patient care workflows, and ultimately improve patient outcomes. However, it is essential \"\n",
    "    \"to address ethical considerations, data privacy concerns, and the need for robust \"\n",
    "    \"validation frameworks to ensure the responsible deployment of these transformative technologies.\"\n",
    ")\n",
    "\n",
    "print('\\n--- Should be AI (listicle / structured tips) ---')\n",
    "predict_text(\n",
    "    \"There are several key strategies to improve your productivity. First, prioritize your tasks \"\n",
    "    \"using the Eisenhower Matrix to distinguish between urgent and important activities. Second, \"\n",
    "    \"implement time-blocking techniques to allocate dedicated periods for focused work. Third, \"\n",
    "    \"minimize distractions by creating a dedicated workspace and utilizing productivity applications. \"\n",
    "    \"Finally, regularly review your progress and adjust your strategies accordingly.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14-download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Download the trained model as a zip ───────────────────────────────────────\n",
    "# Unzip into model-service/model/ and restart the Python service.\n",
    "# The model_loader.py will pick it up automatically.\n",
    "\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "zip_path = '/content/ai-detector-fast-export'\n",
    "shutil.make_archive(zip_path, 'zip', BEST_MODEL_DIR)\n",
    "\n",
    "zip_size = os.path.getsize(zip_path + '.zip') / 1e6\n",
    "print(f'Zip size: {zip_size:.0f} MB')\n",
    "print('Downloading...')\n",
    "files.download(zip_path + '.zip')\n",
    "\n",
    "print()\n",
    "print('After downloading:')\n",
    "print('  1. Unzip into model-service/model/')\n",
    "print('  2. Restart the Python service: uvicorn app:app --host 0.0.0.0 --port 8000 --reload')\n",
    "print('  3. The model_loader.py will load DistilBERT weights + calibrated temperature automatically.')"
   ]
  }
 ]
}
